{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TZ2M_nIqDjT",
        "outputId": "5e3609c8-8b15-4657-f7ea-a89c8fc88a93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.8/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.64.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.11.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (4.25.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from sentence_transformers) (0.14.0+cu116)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.9.24)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import gzip\n",
        "import json\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNznJbC3p1Qd",
        "outputId": "ef3f5b33-c8be-40e0-87f0-60facae207ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "from scipy.cluster.vq import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import json\n",
        "from transformers import RobertaConfig\n",
        "from transformers import RobertaForMaskedLM, RobertaModel\n",
        "from transformers import AdamW"
      ],
      "metadata": {
        "id": "VXK62DQVu0yY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df=pd.read_csv(\"/content/drive/MyDrive/Data/grouped_df.csv\",dtype=str)\n",
        "grouped_df=grouped_df[1:25000]"
      ],
      "metadata": {
        "id": "kOZBirc7kmId"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleStackBERT:\n",
        "    def __init__(self, tokenizer_source): \n",
        "        #Initilize pretrained models, embeddings and vocab files\n",
        "        self.model = RobertaModel.from_pretrained('/content/drive/MyDrive/Data/docberta_dummy_25000')\n",
        "        self.codebook = np.load('/content/drive/MyDrive/Data/tp_codebook_25000.npy')\n",
        "        with open('/content/drive/MyDrive/Data/tp_vocabs_25000.json') as json_file:\n",
        "            self.__vocabs = json.load(json_file)\n",
        "        self.sentenceTransformer = SentenceTransformer('all-mpnet-base-v2')\n",
        "        self.bos_token = self.cls_token = ''\n",
        "        self.eos_token = self.sep_token = ''\n",
        "        self.unk_token = ''\n",
        "        self.pad_token = ''\n",
        "        self.mask_token ='' \n",
        "        self.mask_token_id = self.__vocabs['']\n",
        "\n",
        "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        \n",
        "    def getVocabularySize(self):\n",
        "        return len(self.__vocabs)\n",
        "  \n",
        "    # Splits text into sentences\n",
        "    def textToSentences(self,text):\n",
        "        try:\n",
        "            sentences = re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', text)\n",
        "        except:\n",
        "            sentences = [\"\"]\n",
        "        return sentences\n",
        "\n",
        "    # Splits to whole document into sentences and generates sentence embedding using pretrained SBERT model for \n",
        "    # each sentence and returns a  2D list of those embedding\n",
        "    def createSentencesEmbedding(self, data):\n",
        "        # splitting each text of the dataset into a list of sentences\n",
        "        array = []\n",
        "        if type(data) == str: \n",
        "            array.append(data)\n",
        "        else: \n",
        "            array = data\n",
        "\n",
        "        sentences = [self.textToSentences(text) for text in array]\n",
        "        embeddings = []\n",
        "        for i in range(len(sentences)):\n",
        "            embeddings.append(self.sentenceTransformer.encode(sentences[i]))\n",
        "\n",
        "        return embeddings\n",
        "  \n",
        "    def createTokens(self, list_of_docs, max_length= -1, padding = True, truncation = True): \n",
        "        docs_tokens = [] \n",
        "        doc_embeddings = self.createSentencesEmbedding(list_of_docs)\n",
        "        for doc in doc_embeddings:\n",
        "            cluster_ids = vq(doc,self.codebook)\n",
        "            token_ids = []\n",
        "            attention_mask = []\n",
        "            for cluster in cluster_ids[0]: \n",
        "                token_ids.append(self.__vocabs[str(cluster)])\n",
        "            #If truncation is set, we truncate the tokens above max_length and set padding to false\n",
        "            if truncation and max_length != -1 :\n",
        "                if len(token_ids) + 2 > max_length:\n",
        "                    token_ids = token_ids[:(max_length-2)]\n",
        "                    padding = False\n",
        "\n",
        "            input_ids = [self.__vocabs['']] + token_ids + [self.__vocabs['']]\n",
        "            attention_mask.extend([1] * len(input_ids))\n",
        "            # If padding is set then added padding to the remaining space of max_length \n",
        "            # Because all the input ids are not of the same size and roberta models deal with same sized inputs \n",
        "            if padding:\n",
        "                padding_len = max_length - len(input_ids)\n",
        "                input_ids.extend([self.__vocabs['']] * padding_len)\n",
        "                attention_mask.extend([0] * padding_len)\n",
        "            #creates a list of dictionary of input ids and their attention masks\n",
        "            docs_tokens.append({'input_ids':input_ids, 'attention_mask':attention_mask})\n",
        "\n",
        "        return docs_tokens\n",
        "    #Creates document tensors for all the rows in dataset and returns a tensor of 2D array containing tensors for all documents\n",
        "    def documentTensors(self,dataset,max_length= -1, padding = True, truncation = True):\n",
        "        tensors = []\n",
        "        if type(dataset) is str:\n",
        "            dataset = [dataset]\n",
        "        for d in dataset:\n",
        "            t = torch.tensor([self.createTokens(d,max_length,padding,truncation)[0]['input_ids']])\n",
        "            output = self.model(t).pooler_output.cpu().detach().numpy()\n",
        "            tensors.append(output.reshape((output.shape[1])))\n",
        "        return tensors\n",
        "\n",
        "    #Creates a document vector by concatenating document tensors and sentence embeddings\n",
        "    def documentEmbeddings(self,data, max_length= -1, padding = True, truncation = True):\n",
        "        if type(data) == str:\n",
        "            data = [data]\n",
        "        embeddings = self.sentenceTransformer.encode(data)\n",
        "        tensors = self.documentTensors(data,max_length,padding,truncation)\n",
        "        return np.concatenate((embeddings, tensors), axis=1)"
      ],
      "metadata": {
        "id": "7IzfvxeLunZY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "book_embeddings=np.load('/content/drive/MyDrive/Data/book_embeddings_10000.npy',allow_pickle=True)"
      ],
      "metadata": {
        "id": "3w1qU0OEnYfD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def title(index):\n",
        "    return grouped_df[grouped_df.S_no == index][\"title\"].values[0]"
      ],
      "metadata": {
        "id": "_NQGSUN_q4j1"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.cluster.vq import *\n",
        "# using 100 cluster\n",
        "bookCodebook,_ = kmeans(book_embeddings, 100, 20)"
      ],
      "metadata": {
        "id": "8-8iTNk2qqQe"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cluster, _ = vq(book_embeddings,bookCodebook)"
      ],
      "metadata": {
        "id": "9ZPXf2U0uI8r"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df['cluster'] = cluster"
      ],
      "metadata": {
        "id": "M-bxjSlRuI_y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DoubleStackBERT('doublestackbert_tokenizer')\n"
      ],
      "metadata": {
        "id": "u55BU6UCuJFf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae95961-f207-4028-82e7-d7e3985fa80a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/drive/MyDrive/Data/docberta_dummy_10000 were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/Data/docberta_dummy_10000 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getElementsInTheSameCluster(centroid):\n",
        "    cluster =grouped_df\n",
        "    cluster['embeddings_id'] = [i for i in range(0,book_embeddings.shape[0])]\n",
        "    multiIndex_cluster = cluster.sort_values(by=['cluster'])\n",
        "    multiIndex_cluster = multiIndex_cluster.set_index(['cluster'])\n",
        "    books = multiIndex_cluster.loc[centroid]\n",
        "\n",
        "    ids = np.asarray(books.index)\n",
        "    titles = [t for t in books['title']]\n",
        "    genre=[t for t in books['book_genre']]\n",
        "    description = [d for d in books['description']]\n",
        "    embeddings = [book_embeddings[id] for id in books['embeddings_id']]\n",
        "    similar_books = {'id':ids, 'title':titles, 'description':description,'genre':genre}\n",
        "    return similar_books, embeddings\n"
      ],
      "metadata": {
        "id": "n54uqThrtxZX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate relevance of document based on matching number of book genres\n",
        "def relevance(predicted,expected,total):\n",
        "  child_genres=[\"classic\",'favorite','fiction','children','fantasy','young-adult','childhood','kids','adventure','animal','school','picture-books','library','other']\n",
        "  baseline=sum([1 for i in expected if i==\"1\"])\n",
        "  rel_score=[]\n",
        "  for p in predicted[:total]:\n",
        "    score=0\n",
        "    for idx,p in enumerate(p):\n",
        "      if expected[idx]==p and p==\"1\":score+=1\n",
        "    rel_score.append(score/(baseline+0.01)*1)\n",
        "  return rel_score\n",
        "\n",
        "#calculate precision with a threshold of 0.4\n",
        "def precision(relevance_score):\n",
        "  return sum([1 for i in relevance_score if i>0.4])/len(relevance_score)\n"
      ],
      "metadata": {
        "id": "4gEz6j2myL16"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_score(total=10):\n",
        "  score=[]\n",
        "  for row,d in grouped_df.iterrows():\n",
        "    name=d[\"title\"]\n",
        "    desc=d[\"description\"]\n",
        "    #concatenate book name and description and get document embedding for the same\n",
        "    query_vector=model.getEmbeddings(name+desc)\n",
        "    #check qhich cluster it belongs to\n",
        "    queryCluster, _ = vq(query_vector,bookCodebook)\n",
        "    #Fetch books in the same cluster\n",
        "    similar_books, cluster_book_embeddings = getElementsInTheSameCluster(queryCluster[0])\n",
        "    book_g=d[\"book_genre\"]\n",
        "    similar_books=list(set(similar_books['genre']))\n",
        "    #calculate precision@total\n",
        "    relevance_score=relevance(similar_books,book_g,total)\n",
        "    prec=precision(relevance_score)\n",
        "    score.append(prec)\n",
        "  return sum(score)/len(score)\n",
        "  "
      ],
      "metadata": {
        "id": "WymYvvvg4--M"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=calculate_score(5)\n",
        "print(\"Precision @5: \",result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVo4f93aMLFE",
        "outputId": "654726df-340e-4891-9ed9-db4a0da7fa82"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision @5:  0.9413741374137464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=calculate_score(10)\n",
        "print(\"Precision @10: \",result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHxSmb_tQXnH",
        "outputId": "ada9d6bb-dc0e-4827-bea8-598f89103c10"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision @10:  0.7889888988899418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result=calculate_score(25)\n",
        "print(\"Precision @25: \",result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUpVyr-KU0uu",
        "outputId": "65c9f358-74e1-4090-9e64-4257e877c4ea"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision @25:  0.7621362136214499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KvutCglz_UgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}